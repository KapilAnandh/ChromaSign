**README.md**
# ğŸ–ï¸ Sign Language Recognition & Shirt Color Detection
This project combines **Computer Vision (OpenCV + Mediapipe)** and **Transformers (Hugging Face Models)** to perform two key tasks on a given image:
1. **Sign Language Gesture Recognition** â€” using a pretrained Vision Transformer (ViT) model.
2. **Shirt Color Detection** â€” using semantic segmentation (SegFormer) and color clustering (KMeans).

## ğŸš€ Features
- Detects **human hands and landmarks** using Mediapipe.
- Recognizes **sign language gestures** using `google/vit-base-patch16-224`.
- Identifies the **dominant shirt color** by:
  - Performing **semantic segmentation** with NVIDIAâ€™s `segformer-b0-finetuned-ade-512-512`.
  - Extracting the **torso region**.
  - Applying **KMeans clustering** to find the dominant RGB color.
  - Mapping RGB values to the closest **CSS3 color name**.
- Displays results visually and textually.

## ğŸ§© How It Works

1. **Input:**  
   The user provides an image (e.g., a person performing a sign gesture).

2. **Processing Steps:**
   - **Pose Detection:** Mediapipe estimates human pose landmarks (shoulders, hips, etc.).
   - **Hand Detection:** Mediapipe identifies and draws hand landmarks.
   - **Shirt Segmentation:** SegFormer segments the image to isolate the personâ€™s torso area.
   - **Color Extraction:** KMeans finds the most dominant shirt color.
   - **Gesture Prediction:** The Vision Transformer (ViT) model predicts the sign/gesture class.

3. **Output:**
   - Displays:
     - Hand landmarks overlay on the original image.
     - Detected shirt color patch.
   - Prints:
     - Dominant shirt color name and RGB value.
     - Predicted sign/gesture label.

## ğŸ“¸ Example Output

ğŸ‘• Detected Shirt Color: blue (RGB: (42, 68, 173))
ğŸ–ï¸ Predicted Sign/Gesture Label: Thumbs Up

Visuals:
- **Input Image**
- **Detected Hands Overlay**
- **Detected Shirt Color Patch**

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/sign-language-color-detector.git
cd sign-language-color-detector
### 2ï¸âƒ£ Install Dependencies
Create a virtual environment (recommended):

python -m venv venv
source venv/bin/activate     # (Linux/Mac)
venv\Scripts\activate        # (Windows)
Install all dependencies:

pip install -r requirements.txt
âš™ï¸ Usage
Place your image in the sample_images/ folder, then run:

python app/main.py
To test with a new image:

image_path = "sample_images/your_image.jpg"
**ğŸ“¦ Dependencies**
Python 3.9+
torch
torchvision
transformers
mediapipe
opencv-python-headless
matplotlib
numpy
scikit-learn
pillow
huggingface_hub
(See requirements.txt for exact versions.)

**ğŸ’¡ Where Itâ€™s Useful**
Sign Language Recognition Systems â€” can be extended to detect and classify real-time sign gestures.
Human-Computer Interaction (HCI) â€” color detection + gesture control for accessibility systems.
Surveillance & Analytics â€” detecting specific color codes (e.g., uniform colors).
Education & Research â€” demonstration of multimodal computer vision using segmentation + classification.

ğŸ¤ Contributing
Pull requests are welcome!
If you have suggestions for improvements or want to train on custom gestures, please open an issue or submit a PR.

ğŸ§‘â€ğŸ’» Author
Kapil Anandh
